{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bc7f62-e0cb-4004-9c63-8986ba6b27b5",
   "metadata": {},
   "source": [
    "# Reading the Climate variables\n",
    "\n",
    "In the following we load the climate variables computed at `./weather/climate_vars.ipynb`. We index all the rows based on their `nuts` location and the timestamp of the datapoint. This will allow us to merge it with the SWB data later on.\n",
    "\n",
    "To know from which climate table we want to merge the data, the nuts information has to be provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b6bedb9-e30c-4dc4-8a0d-e38202ca6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sqlalchemy import create_engine, inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55116f36-521f-4f9b-9e07-b515da2d3efe",
   "metadata": {},
   "source": [
    "Reading the climate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cedf9342-1b08-4d15-9387-c9a0fae7183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_path = \"weather/prod/climate.db\"\n",
    "c_engine = create_engine(\"sqlite:///\"+c_path, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e6be6d5-f875-4a00-8456-9ecab9212638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reading only the required data\n",
    "def load_climate_data(con, nuts_lvl):\n",
    "    # read tables in db\n",
    "    insp = inspect(con)\n",
    "    tables = insp.get_table_names()\n",
    "    tables = [x for x in tables if x[0] == str(nuts_lvl)]\n",
    "    # create dataframe\n",
    "    df = pd.DataFrame()\n",
    "    for table in tables:\n",
    "        tmp = pd.read_sql_table(table, con)\n",
    "        tmp[\"table_name\"] = table\n",
    "        if df.empty:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat([df, tmp], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1a0148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_lvl = 1\n",
    "df_climate = load_climate_data(c_engine, nuts_lvl=nuts_lvl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91792cc-ef3e-4e6f-916b-5d1bde9c3f6c",
   "metadata": {},
   "source": [
    "# Reading the SOEP Household data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2653846-12d9-406e-9fa7-0a04f269e70f",
   "metadata": {},
   "source": [
    "Because the individual questionair file is quite big we will merge it chunkwise with the weather and household data. Therefore we first read the hh files. All the required hh data can be found in two different data sets. These are merged in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddb4e8df-9113-4f22-9911-072310b4aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "hbrutto_path = \"./data/SOEP-CORE.v37eu_CSV/CSV/hbrutto.csv\"\n",
    "hl_path = \"./data/SOEP-CORE.v37eu_CSV/CSV/hl.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9852b-a66d-4e02-81b8-c5dd0af3142c",
   "metadata": {},
   "source": [
    "`hbrutto` contains meta data about the interviews. This includes data such as \"day of interview\", \"location\", etc. The `hl` file contains the interview responses of all waves in a long format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "206b7ee1-e64d-4319-badb-771b3df06412",
   "metadata": {},
   "source": [
    "In the following are the columns of interest with in the `hl` and `hbrutto` file with the corresponding meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77010969-2ce6-4900-ad79-8ba7c4c56e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_var = {\n",
    "    \"syear\":\"year\", # survey year -> prim. key\n",
    "    \"hid\":\"hid\", # hh id -> prim. key\n",
    "    # hh control variables:\n",
    "    \"hlc0005_h\":\"hh_income\", #[de] Monatliches HH-Netto-Einkommen [harmonisiert]\n",
    "    \"hlc0043\":\"hh_children\", # Number Children\n",
    "    \"hlf0001_h\":\"homeownership\", # Homewonership\n",
    "    \"hlk0056\":\"interview_type\", # Type of interview\n",
    "} # \n",
    "\n",
    "hbrutto_var = {\n",
    "    \"syear\":\"year\", # survey year -> prim. key\n",
    "    \"hid\":\"hid\", # hh id -> prim. key\n",
    "    \"bula_h\":\"sloc\" # location -> bundesland\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222c497-7097-430a-ab82-0007852ff286",
   "metadata": {},
   "source": [
    "The two files are now read in to their respective dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c2ec0f3-5dba-481e-9ef9-405a47fd885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hl = pd.read_csv(hl_path, usecols=hl_var.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f351c1b0-7d6e-4609-93b7-86f60989d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbrutto = pd.read_csv(hbrutto_path, usecols=hbrutto_var.keys())\n",
    "# change location variable to be able to merge with weather data\n",
    "df_hbrutto.rename({\"bula_h\":\"sloc\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c2812-87bc-47a6-91f4-0a5e581806db",
   "metadata": {},
   "source": [
    "The two dataframes can now be merged on their primary key. According to the documentation of the db the prim. keys are `hid` and `syear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1fab759-5662-4502-955b-f05272a2cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hh = df_hl.merge(df_hbrutto, how=\"inner\", on=[\"hid\", \"syear\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eff551-7a42-43b9-a2c7-03645d62939a",
   "metadata": {},
   "source": [
    "# Reading the SOEP Individual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "829275d5-bffa-488b-862a-5e3b18825329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear db\n",
    "!echo > ./prod/data.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6112267-1e23-4a93-ab9e-0f7c077d9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///prod/data.db\", echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264fa14-2a64-48ad-8382-a719d4a608bf",
   "metadata": {},
   "source": [
    "Now we read the individual data. This dataset includes the target variable *SWB* as well as many other control variables. The datasets that contain the information used in this analysis are `ppathl` (tracking file) and `pl` (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5f2098d-36f8-4798-a465-fd6113dcb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppathl_path = \"./data/SOEP-CORE.v37eu_CSV/CSV/ppathl.csv\"\n",
    "pequiv_path = \"./data/SOEP-CORE.v37eu_CSV/CSV/pequiv.csv\"\n",
    "pl_path = \"./data/SOEP-CORE.v37eu_CSV/CSV/pl.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52c2cd27-a42c-4bd6-9899-b569ee227b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppathl_var = {\n",
    "    \"pid\":\"pid\", # person id -> prim. key\n",
    "    \"syear\":\"year\", #survey year -> prim. key\n",
    "    # relevant covariates\n",
    "    \"sex\":\"sex\", # gender [1] female [2] male\n",
    "    \"gebjahr\":\"birth_year\", # year of birth\n",
    "    \"partner\":\"relationship\", # [0] no partner, [1] spouse, [2] partner,\n",
    "                              # [3] Probably spouse, [4] probably partner\n",
    "                              # NOTE: join 1&3 and 2&4\n",
    "}\n",
    "\n",
    "pequiv_var = {\n",
    "    \"pid\":\"pid\", # person id -> prim. key\n",
    "    \"syear\":\"year\", #survey year -> prim. key\n",
    "    \"d11109\":\"education_years\", #years of education\n",
    "    \"m11124\":\"disability_status\", #Disability status\n",
    "    \"e11103\":\"\" #Labor Participation\n",
    "}\n",
    "\n",
    "pl_var = {\n",
    "    # ids\n",
    "    \"pid\":\"pid\", # person id -> prim. key\n",
    "    \"syear\":\"year\", #survey year -> prim. key\n",
    "    \"hid\":\"hid\", # hh id -> prim. key\n",
    "    # target variable\n",
    "    \"plh0182\":\"swb\", # Current life dsatisfaction [0-10]\n",
    "    # relevant covariates\n",
    "    \"ptagin\":\"day\", #day of interview\n",
    "    \"pmonin\":\"month\", #month of interview\n",
    "    \"plh0171\":\"satisfaction_health\", # Current Health [1-5] (0=schlecht, 10=gut)\n",
    "    \"plb0021\":\"unemployed\", # [2] No [1] Yes\n",
    "    \"plh0173\":\"satisfaction_work\", # [0-10] not satisfied <-> very satisfied, NOTE: many nan\n",
    "    \"plh0174\":\"satisfaction_work_hh\", # same as above (NOTE: maybe take max of both?)\n",
    "    \"plh0175\":\"satisfaction_income\" # Satisfaction With Household Income\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58124a1-3c52-4c6c-bbf6-90846fe30f06",
   "metadata": {},
   "source": [
    "The `ppathl` contains the tracking data of a person. This includes for instance the age or marital status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03cec35e-1be9-47ee-af5d-49d1a3e1a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppathl = pd.read_csv(ppathl_path, usecols=ppathl_var.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "271322be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pequiv = pd.read_csv(pequiv_path, usecols=pequiv_var.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89724dbe-5054-4135-a849-d757419879b9",
   "metadata": {},
   "source": [
    "In the following we merge all the dataframes loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f61636b6-625a-401e-9cf7-dd64f79f43d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722380"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = pd.read_csv(pl_path, usecols=pl_var.keys())\n",
    "\n",
    "## MERGE WITH OTHER DATASETS\n",
    "# merge with tracking data\n",
    "chunk = chunk.merge(df_ppathl, on=[\"syear\", \"pid\"], how=\"inner\")\n",
    "# merge with pequiv (TODO what is this table)\n",
    "chunk = chunk.merge(df_pequiv, on=[\"syear\", \"pid\"], how=\"inner\")\n",
    "# merge with household\n",
    "chunk = chunk.merge(df_hh, on=[\"syear\", \"hid\"], how=\"inner\")\n",
    "\n",
    "## CALCULATE RELEVANT VARIABLES\n",
    "# age:\n",
    "chunk[\"age\"] = chunk[\"syear\"] - chunk[\"gebjahr\"]\n",
    "# time stamp:\n",
    "chunk.rename({'syear':\"year\", 'pmonin':\"month\", 'ptagin':\"day\"}, axis=1, inplace=True)\n",
    "chunk[\"time\"] = pd.to_datetime(chunk[['year', 'month', 'day']], errors='coerce')\n",
    "# drop unuseful columns:\n",
    "chunk.drop(['year', 'month', 'day'], axis=1, inplace=True)\n",
    "# delete invalid time stamps as they cannot be merged with climate data:\n",
    "chunk = chunk[chunk['time'].notna()]\n",
    "\n",
    "## MERGE WITH CLIMATE DF\n",
    "final = pd.merge(chunk, df_climate, on=[\"time\", 'sloc'], how='inner')\n",
    "\n",
    "## SAVE TO DATABASE\n",
    "final.to_csv(\"./prod/1_data.csv\")\n",
    "final.to_sql(f\"{nuts_lvl}_data\", con = engine, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "10416539d8bd7745bfd84da934453afb2f17a286a80737bde1b4f151ea1b0bb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
