{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bc7f62-e0cb-4004-9c63-8986ba6b27b5",
   "metadata": {},
   "source": [
    "# Reading the Climate variables\n",
    "\n",
    "In the following we load the climate variables computed at `./wetter/climate_vars.ipynb`. We index all the rows based on their `nuts` location and the timestamp of the datapoint. This will allow us to merge it with the SWB data later on.\n",
    "\n",
    "To know from which climate table we want to merge the data, the nuts information has to be provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6bedb9-e30c-4dc4-8a0d-e38202ca6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sqlalchemy import create_engine, inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55116f36-521f-4f9b-9e07-b515da2d3efe",
   "metadata": {},
   "source": [
    "Reading the climate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cedf9342-1b08-4d15-9387-c9a0fae7183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_path = \"wetter/prod/climate.db\"\n",
    "c_engine = create_engine(\"sqlite:///\"+c_path, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6be6d5-f875-4a00-8456-9ecab9212638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reading only the required data\n",
    "def load_climate_data(con, nuts_lvl):\n",
    "    # read tables in db\n",
    "    insp = inspect(con)\n",
    "    tables = insp.get_table_names()\n",
    "    tables = [x for x in tables if x[0] == str(nuts_lvl)]\n",
    "    # create dataframe\n",
    "    df = pd.DataFrame()\n",
    "    for table in tables:\n",
    "        tmp = pd.read_sql_table(table, con)\n",
    "        tmp[\"table_name\"] = table\n",
    "        if df.empty:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat([df, tmp], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91792cc-ef3e-4e6f-916b-5d1bde9c3f6c",
   "metadata": {},
   "source": [
    "# Reading the SOEP Household data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2653846-12d9-406e-9fa7-0a04f269e70f",
   "metadata": {},
   "source": [
    "Because the individual questionair file is quite big we will merge it chunkwise with the weather and household data. Therefore we first read the hh files. All the required hh data can be found in two different data sets. These are merged in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb4e8df-9113-4f22-9911-072310b4aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "hbrutto_path = \"./soep/SOEP-CORE.v37eu_CSV/CSV/hbrutto.csv\"\n",
    "hl_path = \"./soep/SOEP-CORE.v37eu_CSV/CSV/hl.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9852b-a66d-4e02-81b8-c5dd0af3142c",
   "metadata": {},
   "source": [
    "`hbrutto` contains meta data about the interviews. This includes data such as \"day of interview\", \"location\", etc. The `hl` file contains the interview responses of all waves in a long format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b7ee1-e64d-4319-badb-771b3df06412",
   "metadata": {},
   "source": [
    "In the following are the columns of interest with in the `hl` and `hbrutto` file with the corresponding meaning. It is written in a dictionary format so we can rename the variables into something more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77010969-2ce6-4900-ad79-8ba7c4c56e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_var = {\n",
    "    \"syear\":\"year\", # survey year -> prim. key\n",
    "    \"hid\":\"hid\", # hh id -> prim. key\n",
    "    # hh control variables:\n",
    "    \"hlc0005_h\":\"hh_einkommen\", #[de] Monatliches HH-Netto-Einkommen [harmonisiert]\n",
    "    \"hlc0043\":\"hh_children\" # Number Children\n",
    "}\n",
    "\n",
    "hbrutto_var = {\n",
    "    \"syear\":\"year\", # survey year -> prim. key\n",
    "    \"hid\":\"hid\", # hh id -> prim. key\n",
    "    \"bula_h\":\"sloc\" # location -> bundesland/kreis/etc\n",
    "    #_:\"sloc\" ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222c497-7097-430a-ab82-0007852ff286",
   "metadata": {},
   "source": [
    "The two files are now read in to their respective dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c2ec0f3-5dba-481e-9ef9-405a47fd885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hl = pd.read_csv(hl_path, usecols=hl_var.keys())\n",
    "df_hl.rename(hl_var, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f351c1b0-7d6e-4609-93b7-86f60989d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbrutto = pd.read_csv(hbrutto_path, usecols=hbrutto_var.keys())\n",
    "df_hbrutto.rename(hbrutto_var, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c2812-87bc-47a6-91f4-0a5e581806db",
   "metadata": {},
   "source": [
    "The two dataframes can now be merged on their primary key. According to the documentation of the db the prim. keys are `hid` and `syear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1fab759-5662-4502-955b-f05272a2cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hh = df_hl.merge(df_hbrutto, how=\"inner\", on=[\"hid\", \"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eff551-7a42-43b9-a2c7-03645d62939a",
   "metadata": {},
   "source": [
    "# Reading the SOEP Individual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829275d5-bffa-488b-862a-5e3b18825329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear db\n",
    "!echo > ./prod/data.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6112267-1e23-4a93-ab9e-0f7c077d9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///prod/data.db\", echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264fa14-2a64-48ad-8382-a719d4a608bf",
   "metadata": {},
   "source": [
    "Now we read the individual data. This dataset includes the target variable *SWB* as well as many other control variables. The datasets that contain the information used in this analysis are `ppathl` (tracking file) and `pl` (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5f2098d-36f8-4798-a465-fd6113dcb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppathl_path = \"./soep/SOEP-CORE.v37eu_CSV/CSV/ppathl.csv\"\n",
    "pl_path = \"./soep/SOEP-CORE.v37eu_CSV/CSV/pl.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c2cd27-a42c-4bd6-9899-b569ee227b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppathl_var = {\n",
    "    \"pid\":\"pid\", # person id -> prim. key\n",
    "    \"syear\":\"year\", #survey year -> prim. key\n",
    "    # relevant covariates\n",
    "    \"sex\":\"gender\", # Current life satisfaction [0-10] TARGET VARIABLE\n",
    "    \"gebjahr\":\"birth_year\", # year of birth\n",
    "    \"partner\":\"relationship\", # [0] no partner, [1] spouse, [2] partner, [3] Probably spouse\n",
    "}\n",
    "\n",
    "pl_var = {\n",
    "    # ids\n",
    "    \"pid\":\"pid\", # person id -> prim. key\n",
    "    \"hid\":\"hid\", # hh id -> forg. key\n",
    "    \"syear\":\"year\", #survey year -> prim. key\n",
    "    \"ptagin\":\"day\", #day of interview\n",
    "    \"pmonin\":\"month\", #month of interview\n",
    "    # target variable\n",
    "    \"plh0182\":\"swb\", # Current life satisfaction [0-10]\n",
    "    # relevant covariates\n",
    "    \"plh0171\":\"health\", # Current Health [1-5] (0=schlecht, 10=gut)\n",
    "    \"plb0021\":\"unemployed\", # [2] No [1] Yes\n",
    "    \"plh0173\":\"work\", # [0-10] not satisfied <-> very satisfied, NOTE: many nan - family?\n",
    "    \"plh0174\":\"work_hh\", # same as above (NOTE: maybe take max of both?)\n",
    "    \"plg0030\":\"education\", # Total Education, Training Item Nonresponse, NOTE: many nan\n",
    "    \"plh0175\":\"income_satisfaction\" # Satisfaction With Household Income\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58124a1-3c52-4c6c-bbf6-90846fe30f06",
   "metadata": {},
   "source": [
    "The `ppathl` contains the tracking data of a person. This includes for instance the age or marital status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03cec35e-1be9-47ee-af5d-49d1a3e1a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ppathl = pd.read_csv(ppathl_path, usecols=ppathl_var.keys())\n",
    "df_ppathl.rename(ppathl_var, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89724dbe-5054-4135-a849-d757419879b9",
   "metadata": {},
   "source": [
    "The `pl` file is the largest in the database. Most machines will not support loading this file into memory. Fortunatiely, a left join needs to be performed on this file such that it is possible to do the merger chunkwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f61636b6-625a-401e-9cf7-dd64f79f43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1000\n",
    "nuts_lvl = 1\n",
    "df_climate = load_climate_data(c_engine, nuts_lvl=nuts_lvl) \n",
    "for chunk in pd.read_csv(pl_path, chunksize=chunksize, usecols=pl_var.keys()):\n",
    "    # rename vars\n",
    "    chunk.rename(pl_var, axis=1, inplace=True)\n",
    "    \n",
    "    ## MERGE WITH OTHER DATASETS\n",
    "    # merge with tracking data\n",
    "    chunk = chunk.merge(df_ppathl, on=[\"year\", \"pid\"], how=\"inner\")\n",
    "    # merge with household\n",
    "    chunk = chunk.merge(df_hh, on=[\"year\", \"hid\"], how=\"inner\")\n",
    "    \n",
    "    ## CALCULATE RELEVANT VARIABLES\n",
    "    # age:\n",
    "    chunk[\"age\"] = chunk[\"year\"] - chunk[\"birth_year\"]\n",
    "    # time stamp:\n",
    "    chunk[\"time\"] = pd.to_datetime(chunk[['year', 'month', 'day']], errors='coerce')\n",
    "    # drop unuseful columns:\n",
    "    chunk.drop(['year', 'month', 'day'], axis=1, inplace=True)\n",
    "    # delete invalid time stamps as they cannot be merged with climate data:\n",
    "    chunk = chunk[chunk['time'].notna()]\n",
    "    \n",
    "    ## MERGE WITH CLIMATE DF\n",
    "    final = pd.merge(chunk, df_climate, on=[\"time\", 'sloc'], how='inner')\n",
    "    \n",
    "    ## SAVE TO DATABASE\n",
    "    final.to_sql(f\"{nuts_lvl}_data\", con = engine, if_exists='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "10416539d8bd7745bfd84da934453afb2f17a286a80737bde1b4f151ea1b0bb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
